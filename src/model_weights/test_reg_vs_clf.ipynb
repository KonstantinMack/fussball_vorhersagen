{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from helper_fcts import preprocess, get_poi_mas, rps\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline\n",
    "from IPython.display import SVG, HTML\n",
    "from operator import itemgetter\n",
    "from scipy.stats import poisson\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.externals import joblib\n",
    "from keras.models import load_model\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from main import modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\scipy\\stats\\_discrete_distns.py:493: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  return mu >= 0\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.concat([modelling(pd.read_csv(\"data/E\" + str(i) + \".csv\"), 20) for i in range(8,19)])\n",
    "#df2 = pd.concat([modelling(pd.read_csv(\"data/F\" + str(i) + \".csv\"), 20) for i in range(8,19)])\n",
    "#df3 = pd.concat([modelling(pd.read_csv(\"data/D\" + str(i) + \".csv\"), 18) for i in range(8,19)])\n",
    "#df4 = pd.concat([modelling(pd.read_csv(\"data/I\" + str(i) + \".csv\"), 20) for i in range(8,19)])\n",
    "#df5 = pd.concat([modelling(pd.read_csv(\"data/SP\" + str(i) + \".csv\"), 20) for i in range(8,19)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"country\"] = \"E\"\n",
    "df2[\"country\"] = \"F\"\n",
    "df3[\"country\"] = \"G\"\n",
    "df4[\"country\"] = \"I\"\n",
    "df5[\"country\"] = \"S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, pd.get_dummies(df.country)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_mit_fe  = ['H_avgGD', 'A_avgGD', 'H_avgS', 'A_avgS', 'H_avgS_c', 'A_avgS_c', 'H_avgST', 'A_avgST', 'H_avgST_c', 'A_avgST_c', 'GoalDiff_last_away', 'GoalDiff_last_home', 'H_xG_PoiMas', 'A_xG_PoiMas', 'H_Form_Tot4', 'A_Form_Tot4', 'H_prob_odds', 'D_prob_odds', 'A_prob_odds']\n",
    "# data2 = df5[df5[\"round\"] > 10].reset_index(drop=True)\n",
    "# data = data2[columns_mit_fe]\n",
    "# data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "columns_mit_fe  = ['Home', 'Draw', 'Away', 'season', 'H_avgGD', 'A_avgGD', 'H_avgS', 'A_avgS', 'H_avgS_c', 'A_avgS_c', 'H_avgST', 'A_avgST', 'H_avgST_c', 'A_avgST_c', 'GoalDiff_last_away', 'GoalDiff_last_home', 'H_xG_PoiMas', 'A_xG_PoiMas', 'H_Form_Tot4', 'A_Form_Tot4', 'H_prob_odds', 'D_prob_odds', 'A_prob_odds']\n",
    "\n",
    "def get_season(df, season):\n",
    "    X_train = df[df[\"season\"] < season].iloc[:,3:]\n",
    "    X_test = df[df[\"season\"] == season].iloc[:,3:]\n",
    "    y_train = df[df[\"season\"] < season].iloc[:,:3]\n",
    "    y_test = df[df[\"season\"] == season].iloc[:,:3]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def loss_rps(y_true, y_pred):\n",
    "    prob_h = y_pred[:, 0]\n",
    "    prob_d = y_pred[:, 1]\n",
    "    home = y_true[:, 0]\n",
    "    draw = y_true[:, 1]\n",
    "\n",
    "    step1 = prob_h - home\n",
    "    step2 = prob_d - draw\n",
    "    summe = step1 + step2\n",
    "    return (step1 ** 2 + summe ** 2) / 2\n",
    "\n",
    "\n",
    "data2 = df5[df5[\"round\"] > 10].reset_index(drop=True)\n",
    "data = data2[columns_mit_fe]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_season(data, 18)\n",
    "\n",
    "del X_train[\"season\"]\n",
    "del X_test[\"season\"]\n",
    "\n",
    "scaler = joblib.load(\"scaler_spa.save\")\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "columns_mit_fe  = ['Home', 'Draw', 'Away', 'season', 'E', 'F', 'G', 'I', 'S', 'H_avgGD', 'A_avgGD', 'H_avgS', 'A_avgS', 'H_avgS_c', 'A_avgS_c', 'H_avgST', 'A_avgST', 'H_avgST_c', 'A_avgST_c', 'GoalDiff_last_away', 'GoalDiff_last_home', 'H_xG_PoiMas', 'A_xG_PoiMas', 'H_Form_Tot4', 'A_Form_Tot4', 'H_prob_odds', 'D_prob_odds', 'A_prob_odds']\n",
    "\n",
    "def get_season(df, season):\n",
    "    X_train = df[df[\"season\"] < season].iloc[:,3:]\n",
    "    X_test = df[df[\"season\"] == season].iloc[:,3:]\n",
    "    y_train = df[df[\"season\"] < season].iloc[:,:3]\n",
    "    y_test = df[df[\"season\"] == season].iloc[:,:3]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def loss_rps(y_true, y_pred):\n",
    "    prob_h = y_pred[:, 0]\n",
    "    prob_d = y_pred[:, 1]\n",
    "    home = y_true[:, 0]\n",
    "    draw = y_true[:, 1]\n",
    "\n",
    "    step1 = prob_h - home\n",
    "    step2 = prob_d - draw\n",
    "    summe = step1 + step2\n",
    "    return (step1 ** 2 + summe ** 2) / 2\n",
    "\n",
    "\n",
    "data2 = df[df[\"round\"] > 10].reset_index(drop=True)\n",
    "data = data2[columns_mit_fe]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_season(data, 18)\n",
    "\n",
    "del X_train[\"season\"]\n",
    "del X_test[\"season\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, kernel_initializer=\"he_normal\", input_shape=(24,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss=loss_rps, optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13343 samples, validate on 1332 samples\n",
      "Epoch 1/100\n",
      "13343/13343 [==============================] - 1s 106us/step - loss: 0.2647 - acc: 0.4298 - val_loss: 0.2103 - val_acc: 0.5278\n",
      "Epoch 2/100\n",
      "13343/13343 [==============================] - 1s 71us/step - loss: 0.2237 - acc: 0.4669 - val_loss: 0.1934 - val_acc: 0.5420\n",
      "Epoch 3/100\n",
      "13343/13343 [==============================] - 1s 70us/step - loss: 0.2068 - acc: 0.4983 - val_loss: 0.1928 - val_acc: 0.5420\n",
      "Epoch 4/100\n",
      "13343/13343 [==============================] - 1s 73us/step - loss: 0.2030 - acc: 0.5185 - val_loss: 0.1923 - val_acc: 0.5420\n",
      "Epoch 5/100\n",
      "13343/13343 [==============================] - 1s 75us/step - loss: 0.2007 - acc: 0.5251 - val_loss: 0.1916 - val_acc: 0.5458\n",
      "Epoch 6/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1998 - acc: 0.5269 - val_loss: 0.1919 - val_acc: 0.5450\n",
      "Epoch 7/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.2000 - acc: 0.5316 - val_loss: 0.1921 - val_acc: 0.5435\n",
      "Epoch 8/100\n",
      "13343/13343 [==============================] - 1s 73us/step - loss: 0.1991 - acc: 0.5296 - val_loss: 0.1912 - val_acc: 0.5480\n",
      "Epoch 9/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1991 - acc: 0.5323 - val_loss: 0.1913 - val_acc: 0.5511\n",
      "Epoch 10/100\n",
      "13343/13343 [==============================] - 1s 81us/step - loss: 0.2000 - acc: 0.5280 - val_loss: 0.1919 - val_acc: 0.5450\n",
      "Epoch 11/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1978 - acc: 0.5325 - val_loss: 0.1915 - val_acc: 0.5450\n",
      "Epoch 12/100\n",
      "13343/13343 [==============================] - 1s 75us/step - loss: 0.1985 - acc: 0.5314 - val_loss: 0.1918 - val_acc: 0.5413\n",
      "Epoch 13/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1982 - acc: 0.5310 - val_loss: 0.1916 - val_acc: 0.5443\n",
      "Epoch 14/100\n",
      "13343/13343 [==============================] - 1s 73us/step - loss: 0.1982 - acc: 0.5325 - val_loss: 0.1913 - val_acc: 0.5435\n",
      "Epoch 15/100\n",
      "13343/13343 [==============================] - 1s 73us/step - loss: 0.1976 - acc: 0.5317 - val_loss: 0.1926 - val_acc: 0.5435\n",
      "Epoch 16/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1979 - acc: 0.5318 - val_loss: 0.1914 - val_acc: 0.5465\n",
      "Epoch 17/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1972 - acc: 0.5313 - val_loss: 0.1927 - val_acc: 0.5450\n",
      "Epoch 18/100\n",
      "13343/13343 [==============================] - 1s 77us/step - loss: 0.1981 - acc: 0.5306 - val_loss: 0.1910 - val_acc: 0.5458\n",
      "Epoch 19/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1977 - acc: 0.5308 - val_loss: 0.1907 - val_acc: 0.5443\n",
      "Epoch 20/100\n",
      "13343/13343 [==============================] - 1s 79us/step - loss: 0.1971 - acc: 0.5327 - val_loss: 0.1911 - val_acc: 0.5420\n",
      "Epoch 21/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1975 - acc: 0.5327 - val_loss: 0.1914 - val_acc: 0.5450\n",
      "Epoch 22/100\n",
      "13343/13343 [==============================] - 1s 77us/step - loss: 0.1971 - acc: 0.5329 - val_loss: 0.1911 - val_acc: 0.5473\n",
      "Epoch 23/100\n",
      "13343/13343 [==============================] - 1s 84us/step - loss: 0.1975 - acc: 0.5321 - val_loss: 0.1914 - val_acc: 0.5420\n",
      "Epoch 24/100\n",
      "13343/13343 [==============================] - 1s 77us/step - loss: 0.1966 - acc: 0.5317 - val_loss: 0.1925 - val_acc: 0.5465\n",
      "Epoch 25/100\n",
      "13343/13343 [==============================] - 1s 77us/step - loss: 0.1969 - acc: 0.5362 - val_loss: 0.1913 - val_acc: 0.5420\n",
      "Epoch 26/100\n",
      "13343/13343 [==============================] - 1s 78us/step - loss: 0.1974 - acc: 0.5343 - val_loss: 0.1919 - val_acc: 0.5435\n",
      "Epoch 27/100\n",
      "13343/13343 [==============================] - 1s 82us/step - loss: 0.1970 - acc: 0.5358 - val_loss: 0.1919 - val_acc: 0.5473\n",
      "Epoch 28/100\n",
      "13343/13343 [==============================] - 1s 75us/step - loss: 0.1975 - acc: 0.5334 - val_loss: 0.1922 - val_acc: 0.5345\n",
      "Epoch 29/100\n",
      "13343/13343 [==============================] - 1s 78us/step - loss: 0.1974 - acc: 0.5346 - val_loss: 0.1918 - val_acc: 0.5398\n",
      "Epoch 30/100\n",
      "13343/13343 [==============================] - 1s 75us/step - loss: 0.1974 - acc: 0.5305 - val_loss: 0.1918 - val_acc: 0.5450\n",
      "Epoch 31/100\n",
      "13343/13343 [==============================] - 1s 79us/step - loss: 0.1968 - acc: 0.5335 - val_loss: 0.1911 - val_acc: 0.5390\n",
      "Epoch 32/100\n",
      "13343/13343 [==============================] - 1s 78us/step - loss: 0.1967 - acc: 0.5329 - val_loss: 0.1919 - val_acc: 0.5398\n",
      "Epoch 33/100\n",
      "13343/13343 [==============================] - 1s 78us/step - loss: 0.1971 - acc: 0.5329 - val_loss: 0.1909 - val_acc: 0.5383\n",
      "Epoch 34/100\n",
      "13343/13343 [==============================] - 1s 78us/step - loss: 0.1962 - acc: 0.5353 - val_loss: 0.1916 - val_acc: 0.5435\n",
      "Epoch 35/100\n",
      "13343/13343 [==============================] - 1s 78us/step - loss: 0.1963 - acc: 0.5373 - val_loss: 0.1917 - val_acc: 0.5443\n",
      "Epoch 36/100\n",
      "13343/13343 [==============================] - 1s 80us/step - loss: 0.1969 - acc: 0.5333 - val_loss: 0.1923 - val_acc: 0.5420\n",
      "Epoch 37/100\n",
      "13343/13343 [==============================] - 1s 78us/step - loss: 0.1961 - acc: 0.5377 - val_loss: 0.1923 - val_acc: 0.5420\n",
      "Epoch 38/100\n",
      "13343/13343 [==============================] - 1s 76us/step - loss: 0.1960 - acc: 0.5377 - val_loss: 0.1913 - val_acc: 0.5398\n",
      "Epoch 39/100\n",
      "13343/13343 [==============================] - 1s 79us/step - loss: 0.1962 - acc: 0.5348 - val_loss: 0.1919 - val_acc: 0.5435\n",
      "Epoch 40/100\n",
      "13343/13343 [==============================] - 1s 77us/step - loss: 0.1963 - acc: 0.5370 - val_loss: 0.1916 - val_acc: 0.5428\n",
      "Epoch 41/100\n",
      "13343/13343 [==============================] - 1s 83us/step - loss: 0.1964 - acc: 0.5361 - val_loss: 0.1923 - val_acc: 0.5428\n",
      "Epoch 42/100\n",
      "13343/13343 [==============================] - 1s 83us/step - loss: 0.1965 - acc: 0.5345 - val_loss: 0.1917 - val_acc: 0.5413\n",
      "Epoch 43/100\n",
      "13343/13343 [==============================] - 1s 79us/step - loss: 0.1966 - acc: 0.5358 - val_loss: 0.1916 - val_acc: 0.5405\n",
      "Epoch 44/100\n",
      "13343/13343 [==============================] - 1s 107us/step - loss: 0.1958 - acc: 0.5386 - val_loss: 0.1909 - val_acc: 0.5413\n",
      "Epoch 45/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1954 - acc: 0.5371 - val_loss: 0.1919 - val_acc: 0.5405\n",
      "Epoch 46/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1954 - acc: 0.5401 - val_loss: 0.1908 - val_acc: 0.5420\n",
      "Epoch 47/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1953 - acc: 0.5365 - val_loss: 0.1917 - val_acc: 0.5375\n",
      "Epoch 48/100\n",
      "13343/13343 [==============================] - 1s 66us/step - loss: 0.1957 - acc: 0.5362 - val_loss: 0.1929 - val_acc: 0.5405\n",
      "Epoch 49/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1955 - acc: 0.5358 - val_loss: 0.1924 - val_acc: 0.5405\n",
      "Epoch 50/100\n",
      "13343/13343 [==============================] - 1s 70us/step - loss: 0.1957 - acc: 0.5356 - val_loss: 0.1912 - val_acc: 0.5390\n",
      "Epoch 51/100\n",
      "13343/13343 [==============================] - 1s 63us/step - loss: 0.1963 - acc: 0.5354 - val_loss: 0.1909 - val_acc: 0.5398\n",
      "Epoch 52/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1967 - acc: 0.5343 - val_loss: 0.1922 - val_acc: 0.5375\n",
      "Epoch 53/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1956 - acc: 0.5371 - val_loss: 0.1910 - val_acc: 0.5405\n",
      "Epoch 54/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1964 - acc: 0.5344 - val_loss: 0.1915 - val_acc: 0.5420\n",
      "Epoch 55/100\n",
      "13343/13343 [==============================] - 1s 78us/step - loss: 0.1961 - acc: 0.5362 - val_loss: 0.1915 - val_acc: 0.5420\n",
      "Epoch 56/100\n",
      "13343/13343 [==============================] - 1s 62us/step - loss: 0.1951 - acc: 0.5370 - val_loss: 0.1915 - val_acc: 0.5383\n",
      "Epoch 57/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1956 - acc: 0.5371 - val_loss: 0.1928 - val_acc: 0.5360\n",
      "Epoch 58/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1957 - acc: 0.5368 - val_loss: 0.1917 - val_acc: 0.5405\n",
      "Epoch 59/100\n",
      "13343/13343 [==============================] - 1s 60us/step - loss: 0.1947 - acc: 0.5405 - val_loss: 0.1919 - val_acc: 0.5375\n",
      "Epoch 60/100\n",
      "13343/13343 [==============================] - 1s 62us/step - loss: 0.1950 - acc: 0.5395 - val_loss: 0.1918 - val_acc: 0.5375\n",
      "Epoch 61/100\n",
      "13343/13343 [==============================] - 1s 90us/step - loss: 0.1947 - acc: 0.5402 - val_loss: 0.1931 - val_acc: 0.5375\n",
      "Epoch 62/100\n",
      "13343/13343 [==============================] - 1s 69us/step - loss: 0.1958 - acc: 0.5361 - val_loss: 0.1909 - val_acc: 0.5435\n",
      "Epoch 63/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1957 - acc: 0.5400 - val_loss: 0.1906 - val_acc: 0.5435\n",
      "Epoch 64/100\n",
      "13343/13343 [==============================] - 1s 66us/step - loss: 0.1944 - acc: 0.5390 - val_loss: 0.1910 - val_acc: 0.5443\n",
      "Epoch 65/100\n",
      "13343/13343 [==============================] - 1s 99us/step - loss: 0.1953 - acc: 0.5372 - val_loss: 0.1919 - val_acc: 0.5428\n",
      "Epoch 66/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1949 - acc: 0.5388 - val_loss: 0.1914 - val_acc: 0.5413\n",
      "Epoch 67/100\n",
      "13343/13343 [==============================] - 1s 74us/step - loss: 0.1946 - acc: 0.5408 - val_loss: 0.1910 - val_acc: 0.5390\n",
      "Epoch 68/100\n",
      "13343/13343 [==============================] - 1s 59us/step - loss: 0.1959 - acc: 0.5365 - val_loss: 0.1916 - val_acc: 0.5435\n",
      "Epoch 69/100\n",
      "13343/13343 [==============================] - 1s 58us/step - loss: 0.1948 - acc: 0.5400 - val_loss: 0.1921 - val_acc: 0.5405\n",
      "Epoch 70/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1952 - acc: 0.5392 - val_loss: 0.1913 - val_acc: 0.5405\n",
      "Epoch 71/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1949 - acc: 0.5354 - val_loss: 0.1914 - val_acc: 0.5413\n",
      "Epoch 72/100\n",
      "13343/13343 [==============================] - 1s 62us/step - loss: 0.1951 - acc: 0.5407 - val_loss: 0.1920 - val_acc: 0.5390\n",
      "Epoch 73/100\n",
      "13343/13343 [==============================] - 1s 59us/step - loss: 0.1950 - acc: 0.5409 - val_loss: 0.1927 - val_acc: 0.5428\n",
      "Epoch 74/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1949 - acc: 0.5400 - val_loss: 0.1918 - val_acc: 0.5413\n",
      "Epoch 75/100\n",
      "13343/13343 [==============================] - 1s 63us/step - loss: 0.1942 - acc: 0.5413 - val_loss: 0.1924 - val_acc: 0.5375\n",
      "Epoch 76/100\n",
      "13343/13343 [==============================] - 1s 59us/step - loss: 0.1950 - acc: 0.5397 - val_loss: 0.1926 - val_acc: 0.5420\n",
      "Epoch 77/100\n",
      "13343/13343 [==============================] - 1s 60us/step - loss: 0.1940 - acc: 0.5405 - val_loss: 0.1922 - val_acc: 0.5360\n",
      "Epoch 78/100\n",
      "13343/13343 [==============================] - 1s 72us/step - loss: 0.1949 - acc: 0.5363 - val_loss: 0.1919 - val_acc: 0.5405\n",
      "Epoch 79/100\n",
      "13343/13343 [==============================] - 1s 69us/step - loss: 0.1944 - acc: 0.5382 - val_loss: 0.1918 - val_acc: 0.5413\n",
      "Epoch 80/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1950 - acc: 0.5409 - val_loss: 0.1922 - val_acc: 0.5368\n",
      "Epoch 81/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1941 - acc: 0.5401 - val_loss: 0.1918 - val_acc: 0.5353\n",
      "Epoch 82/100\n",
      "13343/13343 [==============================] - 1s 73us/step - loss: 0.1947 - acc: 0.5402 - val_loss: 0.1925 - val_acc: 0.5338\n",
      "Epoch 83/100\n",
      "13343/13343 [==============================] - 1s 65us/step - loss: 0.1941 - acc: 0.5423 - val_loss: 0.1929 - val_acc: 0.5368\n",
      "Epoch 84/100\n",
      "13343/13343 [==============================] - 1s 66us/step - loss: 0.1942 - acc: 0.5377 - val_loss: 0.1912 - val_acc: 0.5375\n",
      "Epoch 85/100\n",
      "13343/13343 [==============================] - 1s 59us/step - loss: 0.1941 - acc: 0.5418 - val_loss: 0.1931 - val_acc: 0.5390\n",
      "Epoch 86/100\n",
      "13343/13343 [==============================] - 1s 69us/step - loss: 0.1943 - acc: 0.5431 - val_loss: 0.1918 - val_acc: 0.5368\n",
      "Epoch 87/100\n",
      "13343/13343 [==============================] - 1s 60us/step - loss: 0.1950 - acc: 0.5392 - val_loss: 0.1929 - val_acc: 0.5360\n",
      "Epoch 88/100\n",
      "13343/13343 [==============================] - 1s 59us/step - loss: 0.1936 - acc: 0.5390 - val_loss: 0.1932 - val_acc: 0.5398\n",
      "Epoch 89/100\n",
      "13343/13343 [==============================] - 1s 59us/step - loss: 0.1943 - acc: 0.5395 - val_loss: 0.1942 - val_acc: 0.5398\n",
      "Epoch 90/100\n",
      "13343/13343 [==============================] - 1s 57us/step - loss: 0.1943 - acc: 0.5401 - val_loss: 0.1926 - val_acc: 0.5405\n",
      "Epoch 91/100\n",
      "13343/13343 [==============================] - 1s 71us/step - loss: 0.1941 - acc: 0.5396 - val_loss: 0.1920 - val_acc: 0.5360\n",
      "Epoch 92/100\n",
      "13343/13343 [==============================] - 1s 64us/step - loss: 0.1945 - acc: 0.5379 - val_loss: 0.1912 - val_acc: 0.5390\n",
      "Epoch 93/100\n",
      "13343/13343 [==============================] - 1s 61us/step - loss: 0.1946 - acc: 0.5374 - val_loss: 0.1931 - val_acc: 0.5360\n",
      "Epoch 94/100\n",
      "13343/13343 [==============================] - 1s 59us/step - loss: 0.1935 - acc: 0.5410 - val_loss: 0.1917 - val_acc: 0.5398\n",
      "Epoch 95/100\n",
      "13343/13343 [==============================] - 1s 56us/step - loss: 0.1939 - acc: 0.5431 - val_loss: 0.1913 - val_acc: 0.5375\n",
      "Epoch 96/100\n",
      "13343/13343 [==============================] - 1s 58us/step - loss: 0.1940 - acc: 0.5420 - val_loss: 0.1916 - val_acc: 0.5413\n",
      "Epoch 97/100\n",
      "13343/13343 [==============================] - 1s 60us/step - loss: 0.1937 - acc: 0.5402 - val_loss: 0.1931 - val_acc: 0.5375\n",
      "Epoch 98/100\n",
      "13343/13343 [==============================] - 1s 61us/step - loss: 0.1938 - acc: 0.5410 - val_loss: 0.1914 - val_acc: 0.5390\n",
      "Epoch 99/100\n",
      "13343/13343 [==============================] - 1s 58us/step - loss: 0.1936 - acc: 0.5395 - val_loss: 0.1918 - val_acc: 0.5360\n",
      "Epoch 100/100\n",
      "13343/13343 [==============================] - 1s 59us/step - loss: 0.1937 - acc: 0.5415 - val_loss: 0.1922 - val_acc: 0.5375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24d31846a20>"
      ]
     },
     "execution_count": 948,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Train Dataset:\n",
      " RPS: 0.1882, Accuracy: 0.5522\n",
      "--------------------------------------------\n",
      "Test Dataset:\n",
      " RPS: 0.1922, Accuracy: 0.5375\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------\")\n",
    "print(f\"Train Dataset:\\n RPS: {np.round(model.evaluate(X_train, y_train, verbose=0)[0],4)}, Accuracy: {np.round(model.evaluate(X_train, y_train, verbose=0)[1],4)}\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(f\"Test Dataset:\\n RPS: {np.round(model.evaluate(X_test, y_test, verbose=0)[0],4)}, Accuracy: {np.round(model.evaluate(X_test, y_test, verbose=0)[1],4)}\")\n",
    "print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = data2.iloc[data[data[\"season\"] == 18].index]\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "#clf_pred = pd.DataFrame(prediction, columns = [\"A_pred_Clf\", \"D_pred_Clf\", \"H_pred_Clf\"])\n",
    "ann_pred = pd.DataFrame(y_pred, columns = [\"H_pred_Ann\", \"D_pred_Ann\", \"A_pred_Ann\"])\n",
    "test_df2 = pd.concat([test_df, ann_pred], axis=1)\n",
    "#test_df2[\"rps_Clf\"] = rps(test_df2[\"H_pred_Clf\"], test_df2[\"D_pred_Clf\"], test_df2[\"A_pred_Clf\"], test_df2[\"Home\"], test_df2[\"Draw\"], test_df2[\"Away\"])\n",
    "test_df2[\"rps_ANN\"] = rps(test_df2[\"H_pred_Ann\"], test_df2[\"D_pred_Ann\"], test_df2[\"A_pred_Ann\"], test_df2[\"Home\"], test_df2[\"Draw\"], test_df2[\"Away\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "Bookmaker RPS: 0.1904\n",
      "--------------------------------------------\n",
      "Neural Network RPS: 0.1922\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------------------------------\")\n",
    "print(f\"Bookmaker RPS: {np.round(test_df2['rps_Book'].mean(), 4)}\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(f\"Neural Network RPS: {np.round(test_df2['rps_ANN'].mean(), 4)}\")\n",
    "print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bets on Home Teams in 1X2 market:\n",
      "\n",
      "Nr of Matches: 1332\n",
      "Nr of Bets: 409\n",
      "Avg. Yield: 3.79%\n",
      "Max winning odds: 11.68\n",
      "======================================================================\n",
      "\n",
      "Bets on Home Teams in asian handicap market:\n",
      "\n",
      "Nr of Matches: 1332\n",
      "Nr of Bets: 409\n",
      "Avg. Yield: -6.71%\n",
      "Max winning odds: 2.36\n"
     ]
    }
   ],
   "source": [
    "a = test_df2[(test_df2[\"H_pred_Ann\"] / test_df2[\"H_prob_odds\"] -1 > 0.1)][\"H_PL\"].describe()\n",
    "print(f\"\\nBets on Home Teams in 1X2 market:\\n\\nNr of Matches: {test_df2.shape[0]}\\nNr of Bets: {int(a['count'])}\\nAvg. Yield: {np.round(a['mean'] * 100, 2)}%\\nMax winning odds: {np.round(a['max'] + 1, 2)}\")\n",
    "print(\"======================================================================\")\n",
    "b = test_df2[(test_df2[\"H_pred_Ann\"] / test_df2[\"H_prob_odds\"] -1 > 0.1)][\"H_Ahc_PL\"].describe()\n",
    "print(f\"\\nBets on Home Teams in asian handicap market:\\n\\nNr of Matches: {test_df2.shape[0]}\\nNr of Bets: {int(b['count'])}\\nAvg. Yield: {np.round(b['mean'] * 100, 2)}%\\nMax winning odds: {np.round(b['max'] + 1, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bets on Away Teams in 1X2 market:\n",
      "\n",
      "Nr of Matches: 1332\n",
      "Nr of Bets: 229\n",
      "Avg. Yield: 7.69%\n",
      "Max winning odds: 14.25\n",
      "======================================================================\n",
      "\n",
      "Bets on Away Teams in asian handicap market:\n",
      "\n",
      "Nr of Matches: 1332\n",
      "Nr of Bets: 229\n",
      "Avg. Yield: 2.23%\n",
      "Max winning odds: 2.26\n"
     ]
    }
   ],
   "source": [
    "c = test_df2[(test_df2[\"A_pred_Ann\"] / test_df2[\"A_prob_odds\"] -1 > 0.1)][\"A_PL\"].describe()\n",
    "print(f\"\\nBets on Away Teams in 1X2 market:\\n\\nNr of Matches: {test_df2.shape[0]}\\nNr of Bets: {int(c['count'])}\\nAvg. Yield: {np.round(c['mean'] * 100, 2)}%\\nMax winning odds: {np.round(c['max'] + 1, 2)}\")\n",
    "print(\"======================================================================\")\n",
    "d = test_df2[(test_df2[\"A_pred_Ann\"] / test_df2[\"A_prob_odds\"] -1 > 0.1)][\"A_Ahc_PL\"].describe()\n",
    "print(f\"\\nBets on Away Teams in asian handicap market:\\n\\nNr of Matches: {test_df2.shape[0]}\\nNr of Bets: {int(d['count'])}\\nAvg. Yield: {np.round(d['mean'] * 100, 2)}%\\nMax winning odds: {np.round(d['max'] + 1, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bets on Draws in 1X2 market:\n",
      "\n",
      "Nr of Matches: 1332\n",
      "Nr of Bets: 318\n",
      "Avg. Yield: 8.01%\n",
      "Max winning odds: 5.65\n"
     ]
    }
   ],
   "source": [
    "e = test_df2[(test_df2[\"D_pred_Ann\"] / test_df2[\"D_prob_odds\"] -1 > 0.1)][\"D_PL\"].describe()\n",
    "print(f\"\\nBets on Draws in 1X2 market:\\n\\nNr of Matches: {test_df2.shape[0]}\\nNr of Bets: {int(e['count'])}\\nAvg. Yield: {np.round(e['mean'] * 100, 2)}%\\nMax winning odds: {np.round(e['max'] + 1, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('clf_spa_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = load_model('clf_eng.h5', custom_objects={'loss_rps': loss_rps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred == y_pred2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "columns_mit_fe  = ['FTHG', 'FTAG', 'season', 'H_avgGD', 'A_avgGD', 'H_avgS', 'A_avgS', 'H_avgS_c', 'A_avgS_c', 'H_avgST', 'A_avgST', 'H_avgST_c', 'A_avgST_c', 'GoalDiff_last_away', 'GoalDiff_last_home', 'H_xG_PoiMas', 'A_xG_PoiMas', 'H_Form_Tot4', 'A_Form_Tot4', 'H_prob_odds', 'D_prob_odds', 'A_prob_odds']\n",
    "\n",
    "def get_season(df, season):\n",
    "    X_train = df[df[\"season\"] < season].iloc[:,2:]\n",
    "    X_test = df[df[\"season\"] == season].iloc[:,2:]\n",
    "    y_train = df[df[\"season\"] < season].iloc[:,:2]\n",
    "    y_test = df[df[\"season\"] == season].iloc[:,:2]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def loss_rps(y_true, y_pred):\n",
    "    prob_h = y_pred[:, 0]\n",
    "    prob_d = y_pred[:, 1]\n",
    "    home = y_true[:, 0]\n",
    "    draw = y_true[:, 1]\n",
    "\n",
    "    step1 = prob_h - home\n",
    "    step2 = prob_d - draw\n",
    "    summe = step1 + step2\n",
    "    return (step1 ** 2 + summe ** 2) / 2\n",
    "\n",
    "\n",
    "data2 = df1[df1[\"round\"] > 10].reset_index(drop=True)\n",
    "data = data2[columns_mit_fe]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_season(data, 18)\n",
    "\n",
    "del X_train[\"season\"]\n",
    "del X_test[\"season\"]\n",
    "\n",
    "scaler = joblib.load(\"scaler_eng.save\")\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "columns_mit_fe  = ['FTHG', 'FTAG', 'season', 'E', 'F', 'G', 'I', 'S', 'H_avgGD', 'A_avgGD', 'H_avgS', 'A_avgS', 'H_avgS_c', 'A_avgS_c', 'H_avgST', 'A_avgST', 'H_avgST_c', 'A_avgST_c', 'GoalDiff_last_away', 'GoalDiff_last_home', 'H_xG_PoiMas', 'A_xG_PoiMas', 'H_Form_Tot4', 'A_Form_Tot4', 'H_prob_odds', 'D_prob_odds', 'A_prob_odds']\n",
    "\n",
    "def get_season(df, season):\n",
    "    X_train = df[df[\"season\"] < season].iloc[:,2:]\n",
    "    X_test = df[df[\"season\"] == season].iloc[:,2:]\n",
    "    y_train = df[df[\"season\"] < season].iloc[:,:2]\n",
    "    y_test = df[df[\"season\"] == season].iloc[:,:2]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def loss_rps(y_true, y_pred):\n",
    "    prob_h = y_pred[:, 0]\n",
    "    prob_d = y_pred[:, 1]\n",
    "    home = y_true[:, 0]\n",
    "    draw = y_true[:, 1]\n",
    "\n",
    "    step1 = prob_h - home\n",
    "    step2 = prob_d - draw\n",
    "    summe = step1 + step2\n",
    "    return (step1 ** 2 + summe ** 2) / 2\n",
    "\n",
    "\n",
    "data2 = df[df[\"round\"] > 10].reset_index(drop=True)\n",
    "data = data2[columns_mit_fe]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_season(data, 18)\n",
    "\n",
    "del X_train[\"season\"]\n",
    "del X_test[\"season\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X = np.concatenate((X_train, X_test))\n",
    "y = np.concatenate((y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(2048, kernel_initializer=\"he_normal\", input_shape=(24,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(loss=\"logcosh\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(32, kernel_initializer=\"he_normal\", input_shape=(19,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "model.compile(loss=\"logcosh\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2798 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "2798/2798 [==============================] - 1s 434us/step - loss: 0.9822 - acc: 0.5558 - val_loss: 0.6753 - val_acc: 0.7179\n",
      "Epoch 2/100\n",
      "2798/2798 [==============================] - 0s 59us/step - loss: 0.7822 - acc: 0.5722 - val_loss: 0.5855 - val_acc: 0.7429\n",
      "Epoch 3/100\n",
      "2798/2798 [==============================] - 0s 54us/step - loss: 0.6725 - acc: 0.5922 - val_loss: 0.5230 - val_acc: 0.7107\n",
      "Epoch 4/100\n",
      "2798/2798 [==============================] - 0s 59us/step - loss: 0.6186 - acc: 0.6094 - val_loss: 0.5032 - val_acc: 0.7214\n",
      "Epoch 5/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.5859 - acc: 0.6233 - val_loss: 0.4874 - val_acc: 0.7536\n",
      "Epoch 6/100\n",
      "2798/2798 [==============================] - 0s 66us/step - loss: 0.5593 - acc: 0.6623 - val_loss: 0.4746 - val_acc: 0.7464\n",
      "Epoch 7/100\n",
      "2798/2798 [==============================] - 0s 58us/step - loss: 0.5420 - acc: 0.6633 - val_loss: 0.4658 - val_acc: 0.7536\n",
      "Epoch 8/100\n",
      "2798/2798 [==============================] - 0s 59us/step - loss: 0.5224 - acc: 0.6894 - val_loss: 0.4638 - val_acc: 0.7536\n",
      "Epoch 9/100\n",
      "2798/2798 [==============================] - 0s 60us/step - loss: 0.5222 - acc: 0.6773 - val_loss: 0.4640 - val_acc: 0.7214\n",
      "Epoch 10/100\n",
      "2798/2798 [==============================] - 0s 55us/step - loss: 0.5051 - acc: 0.6769 - val_loss: 0.4597 - val_acc: 0.7393\n",
      "Epoch 11/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.5016 - acc: 0.6930 - val_loss: 0.4578 - val_acc: 0.7536\n",
      "Epoch 12/100\n",
      "2798/2798 [==============================] - 0s 53us/step - loss: 0.4959 - acc: 0.6880 - val_loss: 0.4537 - val_acc: 0.7500\n",
      "Epoch 13/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.4953 - acc: 0.6980 - val_loss: 0.4548 - val_acc: 0.7464\n",
      "Epoch 14/100\n",
      "2798/2798 [==============================] - 0s 54us/step - loss: 0.4928 - acc: 0.6934 - val_loss: 0.4537 - val_acc: 0.7429\n",
      "Epoch 15/100\n",
      "2798/2798 [==============================] - 0s 61us/step - loss: 0.4836 - acc: 0.7016 - val_loss: 0.4532 - val_acc: 0.7393\n",
      "Epoch 16/100\n",
      "2798/2798 [==============================] - 0s 61us/step - loss: 0.4836 - acc: 0.7001 - val_loss: 0.4511 - val_acc: 0.7250\n",
      "Epoch 17/100\n",
      "2798/2798 [==============================] - 0s 55us/step - loss: 0.4828 - acc: 0.7019 - val_loss: 0.4514 - val_acc: 0.7214\n",
      "Epoch 18/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.4775 - acc: 0.7016 - val_loss: 0.4510 - val_acc: 0.7464\n",
      "Epoch 19/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.4735 - acc: 0.6987 - val_loss: 0.4521 - val_acc: 0.7429\n",
      "Epoch 20/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.4789 - acc: 0.7080 - val_loss: 0.4522 - val_acc: 0.7357\n",
      "Epoch 21/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.4757 - acc: 0.7037 - val_loss: 0.4513 - val_acc: 0.7214\n",
      "Epoch 22/100\n",
      "2798/2798 [==============================] - 0s 57us/step - loss: 0.4803 - acc: 0.7012 - val_loss: 0.4506 - val_acc: 0.7393\n",
      "Epoch 23/100\n",
      "2798/2798 [==============================] - 0s 57us/step - loss: 0.4701 - acc: 0.7069 - val_loss: 0.4497 - val_acc: 0.7250\n",
      "Epoch 24/100\n",
      "2798/2798 [==============================] - 0s 54us/step - loss: 0.4741 - acc: 0.7087 - val_loss: 0.4503 - val_acc: 0.7286\n",
      "Epoch 25/100\n",
      "2798/2798 [==============================] - 0s 62us/step - loss: 0.4688 - acc: 0.7162 - val_loss: 0.4482 - val_acc: 0.7536\n",
      "Epoch 26/100\n",
      "2798/2798 [==============================] - 0s 66us/step - loss: 0.4714 - acc: 0.7094 - val_loss: 0.4489 - val_acc: 0.7321\n",
      "Epoch 27/100\n",
      "2798/2798 [==============================] - 0s 53us/step - loss: 0.4709 - acc: 0.7087 - val_loss: 0.4478 - val_acc: 0.7357\n",
      "Epoch 28/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.4681 - acc: 0.7094 - val_loss: 0.4485 - val_acc: 0.7286\n",
      "Epoch 29/100\n",
      "2798/2798 [==============================] - 0s 61us/step - loss: 0.4700 - acc: 0.7051 - val_loss: 0.4474 - val_acc: 0.7321\n",
      "Epoch 30/100\n",
      "2798/2798 [==============================] - 0s 62us/step - loss: 0.4679 - acc: 0.7066 - val_loss: 0.4468 - val_acc: 0.7357\n",
      "Epoch 31/100\n",
      "2798/2798 [==============================] - 0s 57us/step - loss: 0.4683 - acc: 0.7069 - val_loss: 0.4469 - val_acc: 0.7321\n",
      "Epoch 32/100\n",
      "2798/2798 [==============================] - 0s 57us/step - loss: 0.4671 - acc: 0.7059 - val_loss: 0.4466 - val_acc: 0.7286\n",
      "Epoch 33/100\n",
      "2798/2798 [==============================] - 0s 57us/step - loss: 0.4647 - acc: 0.7112 - val_loss: 0.4474 - val_acc: 0.7214\n",
      "Epoch 34/100\n",
      "2798/2798 [==============================] - 0s 63us/step - loss: 0.4668 - acc: 0.7102 - val_loss: 0.4479 - val_acc: 0.7321\n",
      "Epoch 35/100\n",
      "2798/2798 [==============================] - 0s 61us/step - loss: 0.4662 - acc: 0.7105 - val_loss: 0.4466 - val_acc: 0.7321\n",
      "Epoch 36/100\n",
      "2798/2798 [==============================] - 0s 62us/step - loss: 0.4654 - acc: 0.7105 - val_loss: 0.4474 - val_acc: 0.7250\n",
      "Epoch 37/100\n",
      "2798/2798 [==============================] - 0s 59us/step - loss: 0.4682 - acc: 0.7080 - val_loss: 0.4470 - val_acc: 0.7321\n",
      "Epoch 38/100\n",
      "2798/2798 [==============================] - 0s 57us/step - loss: 0.4633 - acc: 0.7127 - val_loss: 0.4461 - val_acc: 0.7250\n",
      "Epoch 39/100\n",
      "2798/2798 [==============================] - 0s 56us/step - loss: 0.4656 - acc: 0.7084 - val_loss: 0.4469 - val_acc: 0.7286\n",
      "Epoch 40/100\n",
      "2798/2798 [==============================] - 0s 73us/step - loss: 0.4630 - acc: 0.7066 - val_loss: 0.4483 - val_acc: 0.7250\n",
      "Epoch 41/100\n",
      "2798/2798 [==============================] - 0s 92us/step - loss: 0.4666 - acc: 0.7112 - val_loss: 0.4467 - val_acc: 0.7286\n",
      "Epoch 42/100\n",
      "2798/2798 [==============================] - 0s 82us/step - loss: 0.4622 - acc: 0.7080 - val_loss: 0.4473 - val_acc: 0.7214\n",
      "Epoch 43/100\n",
      "2798/2798 [==============================] - 0s 82us/step - loss: 0.4641 - acc: 0.7019 - val_loss: 0.4463 - val_acc: 0.7214\n",
      "Epoch 44/100\n",
      "2798/2798 [==============================] - 0s 77us/step - loss: 0.4610 - acc: 0.7044 - val_loss: 0.4447 - val_acc: 0.7250\n",
      "Epoch 45/100\n",
      "2798/2798 [==============================] - 0s 63us/step - loss: 0.4649 - acc: 0.7105 - val_loss: 0.4459 - val_acc: 0.7179\n",
      "Epoch 46/100\n",
      "2798/2798 [==============================] - 0s 59us/step - loss: 0.4624 - acc: 0.7051 - val_loss: 0.4464 - val_acc: 0.7286\n",
      "Epoch 47/100\n",
      "2798/2798 [==============================] - 0s 66us/step - loss: 0.4627 - acc: 0.7102 - val_loss: 0.4462 - val_acc: 0.7214\n",
      "Epoch 48/100\n",
      "2798/2798 [==============================] - 0s 67us/step - loss: 0.4619 - acc: 0.7066 - val_loss: 0.4470 - val_acc: 0.7286\n",
      "Epoch 49/100\n",
      "2798/2798 [==============================] - 0s 71us/step - loss: 0.4615 - acc: 0.7123 - val_loss: 0.4471 - val_acc: 0.7214\n",
      "Epoch 50/100\n",
      "2798/2798 [==============================] - 0s 65us/step - loss: 0.4571 - acc: 0.7152 - val_loss: 0.4456 - val_acc: 0.7179\n",
      "Epoch 51/100\n",
      "2798/2798 [==============================] - 0s 63us/step - loss: 0.4633 - acc: 0.7076 - val_loss: 0.4469 - val_acc: 0.7179\n",
      "Epoch 52/100\n",
      "2798/2798 [==============================] - 0s 67us/step - loss: 0.4607 - acc: 0.7012 - val_loss: 0.4456 - val_acc: 0.7321\n",
      "Epoch 53/100\n",
      "2798/2798 [==============================] - 0s 67us/step - loss: 0.4593 - acc: 0.7069 - val_loss: 0.4448 - val_acc: 0.7357\n",
      "Epoch 54/100\n",
      "2798/2798 [==============================] - 0s 72us/step - loss: 0.4621 - acc: 0.7080 - val_loss: 0.4469 - val_acc: 0.7464\n",
      "Epoch 55/100\n",
      "2798/2798 [==============================] - 0s 57us/step - loss: 0.4612 - acc: 0.7102 - val_loss: 0.4446 - val_acc: 0.7321\n",
      "Epoch 56/100\n",
      "2798/2798 [==============================] - 0s 88us/step - loss: 0.4603 - acc: 0.7076 - val_loss: 0.4448 - val_acc: 0.7179\n",
      "Epoch 57/100\n",
      "2798/2798 [==============================] - 0s 90us/step - loss: 0.4614 - acc: 0.7109 - val_loss: 0.4451 - val_acc: 0.7357\n",
      "Epoch 58/100\n",
      "2798/2798 [==============================] - 0s 73us/step - loss: 0.4626 - acc: 0.7119 - val_loss: 0.4474 - val_acc: 0.7321\n",
      "Epoch 59/100\n",
      "2798/2798 [==============================] - 0s 62us/step - loss: 0.4589 - acc: 0.7166 - val_loss: 0.4451 - val_acc: 0.7464\n",
      "Epoch 60/100\n",
      "2798/2798 [==============================] - 0s 59us/step - loss: 0.4588 - acc: 0.7109 - val_loss: 0.4458 - val_acc: 0.7357\n",
      "Epoch 61/100\n",
      "2798/2798 [==============================] - 0s 53us/step - loss: 0.4594 - acc: 0.7091 - val_loss: 0.4453 - val_acc: 0.7321\n",
      "Epoch 62/100\n",
      "2798/2798 [==============================] - 0s 52us/step - loss: 0.4595 - acc: 0.7048 - val_loss: 0.4450 - val_acc: 0.7393\n",
      "Epoch 63/100\n",
      "2798/2798 [==============================] - 0s 52us/step - loss: 0.4604 - acc: 0.7127 - val_loss: 0.4455 - val_acc: 0.7393\n",
      "Epoch 64/100\n",
      "2798/2798 [==============================] - 0s 52us/step - loss: 0.4606 - acc: 0.7102 - val_loss: 0.4459 - val_acc: 0.7464\n",
      "Epoch 65/100\n",
      "2798/2798 [==============================] - 0s 52us/step - loss: 0.4602 - acc: 0.7177 - val_loss: 0.4450 - val_acc: 0.7357\n",
      "Epoch 66/100\n",
      "2798/2798 [==============================] - 0s 52us/step - loss: 0.4617 - acc: 0.7048 - val_loss: 0.4442 - val_acc: 0.7357\n",
      "Epoch 67/100\n",
      "2798/2798 [==============================] - 0s 47us/step - loss: 0.4595 - acc: 0.7105 - val_loss: 0.4439 - val_acc: 0.7464\n",
      "Epoch 68/100\n",
      "2798/2798 [==============================] - 0s 55us/step - loss: 0.4613 - acc: 0.7102 - val_loss: 0.4451 - val_acc: 0.7393\n",
      "Epoch 69/100\n",
      "2798/2798 [==============================] - 0s 52us/step - loss: 0.4614 - acc: 0.7091 - val_loss: 0.4448 - val_acc: 0.7500\n",
      "Epoch 70/100\n",
      "2798/2798 [==============================] - 0s 61us/step - loss: 0.4589 - acc: 0.7162 - val_loss: 0.4437 - val_acc: 0.7464\n",
      "Epoch 71/100\n",
      "2798/2798 [==============================] - 0s 52us/step - loss: 0.4594 - acc: 0.7241 - val_loss: 0.4458 - val_acc: 0.7464\n",
      "Epoch 72/100\n",
      "2798/2798 [==============================] - 0s 50us/step - loss: 0.4592 - acc: 0.7130 - val_loss: 0.4460 - val_acc: 0.7357\n",
      "Epoch 73/100\n",
      "2798/2798 [==============================] - 0s 71us/step - loss: 0.4568 - acc: 0.7152 - val_loss: 0.4441 - val_acc: 0.7393\n",
      "Epoch 74/100\n",
      "2798/2798 [==============================] - 0s 93us/step - loss: 0.4577 - acc: 0.7184 - val_loss: 0.4445 - val_acc: 0.7357\n",
      "Epoch 75/100\n",
      "2798/2798 [==============================] - 0s 66us/step - loss: 0.4595 - acc: 0.7062 - val_loss: 0.4437 - val_acc: 0.7357\n",
      "Epoch 76/100\n",
      "2798/2798 [==============================] - 0s 115us/step - loss: 0.4592 - acc: 0.7159 - val_loss: 0.4441 - val_acc: 0.7286\n",
      "Epoch 77/100\n",
      "2798/2798 [==============================] - 0s 115us/step - loss: 0.4583 - acc: 0.7144 - val_loss: 0.4435 - val_acc: 0.7357\n",
      "Epoch 78/100\n",
      "2798/2798 [==============================] - 0s 98us/step - loss: 0.4563 - acc: 0.7144 - val_loss: 0.4458 - val_acc: 0.7500\n",
      "Epoch 79/100\n",
      "2798/2798 [==============================] - 0s 96us/step - loss: 0.4583 - acc: 0.7162 - val_loss: 0.4436 - val_acc: 0.7393\n",
      "Epoch 80/100\n",
      "2798/2798 [==============================] - 0s 90us/step - loss: 0.4583 - acc: 0.7155 - val_loss: 0.4443 - val_acc: 0.7286\n",
      "Epoch 81/100\n",
      "2798/2798 [==============================] - 0s 166us/step - loss: 0.4569 - acc: 0.7180 - val_loss: 0.4445 - val_acc: 0.7393\n",
      "Epoch 82/100\n",
      "2798/2798 [==============================] - 0s 92us/step - loss: 0.4572 - acc: 0.7219 - val_loss: 0.4446 - val_acc: 0.7500\n",
      "Epoch 83/100\n",
      "2798/2798 [==============================] - 0s 82us/step - loss: 0.4561 - acc: 0.7205 - val_loss: 0.4428 - val_acc: 0.7357\n",
      "Epoch 84/100\n",
      "2798/2798 [==============================] - 0s 76us/step - loss: 0.4594 - acc: 0.7130 - val_loss: 0.4441 - val_acc: 0.7393\n",
      "Epoch 85/100\n",
      "2798/2798 [==============================] - 0s 91us/step - loss: 0.4552 - acc: 0.7134 - val_loss: 0.4427 - val_acc: 0.7286\n",
      "Epoch 86/100\n",
      "2798/2798 [==============================] - 0s 71us/step - loss: 0.4585 - acc: 0.7148 - val_loss: 0.4427 - val_acc: 0.7357\n",
      "Epoch 87/100\n",
      "2798/2798 [==============================] - 0s 66us/step - loss: 0.4594 - acc: 0.7169 - val_loss: 0.4426 - val_acc: 0.7429\n",
      "Epoch 88/100\n",
      "2798/2798 [==============================] - 0s 68us/step - loss: 0.4563 - acc: 0.7219 - val_loss: 0.4442 - val_acc: 0.7500\n",
      "Epoch 89/100\n",
      "2798/2798 [==============================] - 0s 62us/step - loss: 0.4577 - acc: 0.7194 - val_loss: 0.4429 - val_acc: 0.7357\n",
      "Epoch 90/100\n",
      "2798/2798 [==============================] - 0s 61us/step - loss: 0.4582 - acc: 0.7094 - val_loss: 0.4437 - val_acc: 0.7500\n",
      "Epoch 91/100\n",
      "2798/2798 [==============================] - 0s 115us/step - loss: 0.4585 - acc: 0.7169 - val_loss: 0.4422 - val_acc: 0.7357\n",
      "Epoch 92/100\n",
      "2798/2798 [==============================] - 0s 96us/step - loss: 0.4563 - acc: 0.7166 - val_loss: 0.4426 - val_acc: 0.7429\n",
      "Epoch 93/100\n",
      "2798/2798 [==============================] - 0s 96us/step - loss: 0.4575 - acc: 0.7155 - val_loss: 0.4431 - val_acc: 0.7321\n",
      "Epoch 94/100\n",
      "2798/2798 [==============================] - 0s 99us/step - loss: 0.4564 - acc: 0.7219 - val_loss: 0.4433 - val_acc: 0.7357\n",
      "Epoch 95/100\n",
      "2798/2798 [==============================] - 0s 83us/step - loss: 0.4609 - acc: 0.7080 - val_loss: 0.4438 - val_acc: 0.7214\n",
      "Epoch 96/100\n",
      "2798/2798 [==============================] - 0s 70us/step - loss: 0.4616 - acc: 0.7127 - val_loss: 0.4427 - val_acc: 0.7357\n",
      "Epoch 97/100\n",
      "2798/2798 [==============================] - 0s 84us/step - loss: 0.4586 - acc: 0.7094 - val_loss: 0.4431 - val_acc: 0.7286\n",
      "Epoch 98/100\n",
      "2798/2798 [==============================] - 0s 97us/step - loss: 0.4585 - acc: 0.7137 - val_loss: 0.4429 - val_acc: 0.7250\n",
      "Epoch 99/100\n",
      "2798/2798 [==============================] - 0s 75us/step - loss: 0.4579 - acc: 0.7076 - val_loss: 0.4436 - val_acc: 0.7286\n",
      "Epoch 100/100\n",
      "2798/2798 [==============================] - 0s 81us/step - loss: 0.4551 - acc: 0.7148 - val_loss: 0.4428 - val_acc: 0.7250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4a4316c18>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      " RPS: 0.4479, Accuracy: 0.7202\n",
      "--------------------------------------------\n",
      "Test Dataset:\n",
      " RPS: 0.4428, Accuracy: 0.725\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Dataset:\\n RPS: {np.round(model.evaluate(X_train, y_train, verbose=0)[0],4)}, Accuracy: {np.round(model.evaluate(X_train, y_train, verbose=0)[1],4)}\")\n",
    "print(\"--------------------------------------------\")\n",
    "print(f\"Test Dataset:\\n RPS: {np.round(model.evaluate(X_test, y_test, verbose=0)[0],4)}, Accuracy: {np.round(model.evaluate(X_test, y_test, verbose=0)[1],4)}\")\n",
    "print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = data2.iloc[data[data[\"season\"] == 18].index]\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "#clf_pred = pd.DataFrame(prediction, columns = [\"A_pred_Clf\", \"D_pred_Clf\", \"H_pred_Clf\"])\n",
    "ann_pred = pd.DataFrame(y_pred, columns = [\"H_pred_Goals\", \"A_pred_Goals\"])\n",
    "test_df2 = pd.concat([test_df, ann_pred], axis=1)\n",
    "#test_df2[\"rps_Clf\"] = rps(test_df2[\"H_pred_Clf\"], test_df2[\"D_pred_Clf\"], test_df2[\"A_pred_Clf\"], test_df2[\"Home\"], test_df2[\"Draw\"], test_df2[\"Away\"])\n",
    "#test_df2[\"rps_ANN\"] = rps(test_df2[\"H_pred_Ann\"], test_df2[\"D_pred_Ann\"], test_df2[\"A_pred_Ann\"], test_df2[\"Home\"], test_df2[\"Draw\"], test_df2[\"Away\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AC', 'AF', 'AR', 'AS', 'AST', 'AY', 'A_Ahc_PL', 'A_Def_Rat',\n",
       "       'A_Form_Tot4', 'A_Off_Rat',\n",
       "       ...\n",
       "       'WHH', 'round', 'rps_Book', 'rps_Mas', 'rps_PoiMas', 'rps_Poi_mix',\n",
       "       'season', 'xGD_Mas', 'H_pred_Goals', 'A_pred_Goals'],\n",
       "      dtype='object', length=180)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2[\"ANN_pred_hc\"] = test_df2[\"A_pred_Goals\"] - test_df2[\"H_pred_Goals\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2[\"Hc_Diff\"] = test_df2[\"BbAHh\"] - test_df2[\"ANN_pred_hc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    129.000000\n",
       " mean       0.006783\n",
       " std        0.836155\n",
       " min       -1.000000\n",
       " 25%       -1.000000\n",
       " 50%        0.325000\n",
       " 75%        0.830000\n",
       " max        1.170000\n",
       " Name: H_Ahc_PL, dtype: float64, count    129.000000\n",
       " mean       0.035349\n",
       " std        1.639838\n",
       " min       -1.000000\n",
       " 25%       -1.000000\n",
       " 50%       -1.000000\n",
       " 75%        0.810000\n",
       " max       10.680000\n",
       " Name: H_PL, dtype: float64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2[test_df2[\"Hc_Diff\"] > 0.05].H_Ahc_PL.describe(), test_df2[test_df2[\"Hc_Diff\"] > 0.05].H_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    114.000000\n",
       " mean       0.026316\n",
       " std        0.835636\n",
       " min       -1.000000\n",
       " 25%       -1.000000\n",
       " 50%        0.382500\n",
       " 75%        0.810000\n",
       " max        1.140000\n",
       " Name: A_Ahc_PL, dtype: float64, count    114.000000\n",
       " mean      -0.039386\n",
       " std        2.252422\n",
       " min       -1.000000\n",
       " 25%       -1.000000\n",
       " 50%       -1.000000\n",
       " 75%        0.515000\n",
       " max       17.000000\n",
       " Name: A_PL, dtype: float64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2[test_df2[\"Hc_Diff\"] < -0.05].A_Ahc_PL.describe(), test_df2[test_df2[\"Hc_Diff\"] < -0.05].A_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('reg_efgis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = load_model('reg_spa.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = model3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "round\n",
       "11    -3.290\n",
       "12     1.300\n",
       "13     2.220\n",
       "14     3.195\n",
       "15    -5.100\n",
       "16     1.490\n",
       "17    -0.355\n",
       "18     3.445\n",
       "19    -1.785\n",
       "20    -2.615\n",
       "21    -3.730\n",
       "22     3.540\n",
       "23     0.355\n",
       "24    -7.280\n",
       "25    -1.480\n",
       "26     3.365\n",
       "27    10.060\n",
       "28    -3.135\n",
       "29     3.040\n",
       "30    -3.790\n",
       "31    -0.560\n",
       "32    -1.570\n",
       "33     0.190\n",
       "34    -1.535\n",
       "35     2.230\n",
       "36    -3.475\n",
       "37     4.260\n",
       "38     1.855\n",
       "Name: H_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 972,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2[test_df2[\"Hc_Diff\"] > 0.1].groupby(\"round\")[\"H_Ahc_PL\"].sum()#[[\"HomeTeam\", \"AwayTeam\", \"FTHG\", \"FTAG\", \"H_pred_Goals\", \"A_pred_Goals\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "round\n",
       "11    2.570\n",
       "12    0.300\n",
       "13    5.790\n",
       "14    3.150\n",
       "15    2.720\n",
       "16   -0.650\n",
       "17    4.595\n",
       "18    6.550\n",
       "19   -0.390\n",
       "20    0.000\n",
       "21    0.260\n",
       "22    1.630\n",
       "23   -1.460\n",
       "24   -1.020\n",
       "25    1.010\n",
       "26   -2.770\n",
       "27    0.165\n",
       "28    1.475\n",
       "29   -1.065\n",
       "30   -0.175\n",
       "31   -1.255\n",
       "32   -1.600\n",
       "33    0.110\n",
       "34    4.010\n",
       "35   -0.195\n",
       "36    0.165\n",
       "37    5.295\n",
       "38    2.225\n",
       "Name: A_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 973,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2[test_df2[\"Hc_Diff\"] < -0.1].groupby(\"round\")[\"A_Ahc_PL\"].sum()#[[\"HomeTeam\", \"AwayTeam\", \"FTHG\", \"FTAG\", \"H_pred_Goals\", \"A_pred_Goals\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AC', 'AF', 'AR', 'AS', 'AST', 'AY', 'A_Ahc_PL', 'A_Def_Rat',\n",
       "       'A_Form_Tot4', 'A_Off_Rat',\n",
       "       ...\n",
       "       'rps_Book', 'rps_Mas', 'rps_PoiMas', 'rps_Poi_mix', 'season', 'xGD_Mas',\n",
       "       'H_pred_Goals', 'A_pred_Goals', 'ANN_pred_hc', 'Hc_Diff'],\n",
       "      dtype='object', length=182)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E\n",
       "0   -3.840\n",
       "1    9.055\n",
       "Name: H_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 992,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2[test_df2[\"Hc_Diff\"] > 0.05].groupby(\"\")[\"H_Ahc_PL\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.95499999999999"
      ]
     },
     "execution_count": 989,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df2[test_df2[\"Hc_Diff\"] < -0.05][\"A_Ahc_PL\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "columns_mit_fe  = ['FTHG', 'FTAG', 'season', 'H_avgGD', 'A_avgGD', 'H_avgS', 'A_avgS', 'H_avgS_c', 'A_avgS_c', 'H_avgST', 'A_avgST', 'H_avgST_c', 'A_avgST_c', 'GoalDiff_last_away', 'GoalDiff_last_home', 'H_xG_PoiMas', 'A_xG_PoiMas', 'H_Form_Tot4', 'A_Form_Tot4', 'H_prob_odds', 'D_prob_odds', 'A_prob_odds']\n",
    "\n",
    "def get_season(df, season):\n",
    "    X_train = df[df[\"season\"] < season].iloc[:,2:]\n",
    "    X_test = df[df[\"season\"] == season].iloc[:,2:]\n",
    "    y_train_hg = df[df[\"season\"] < season].iloc[:,0]\n",
    "    y_test_hg = df[df[\"season\"] == season].iloc[:,0]\n",
    "    y_train_ag = df[df[\"season\"] < season].iloc[:,1]\n",
    "    y_test_ag = df[df[\"season\"] == season].iloc[:,1]\n",
    "    return X_train, X_test, y_train_hg, y_test_hg, y_train_ag, y_test_ag\n",
    "\n",
    "data2 = df1[df1[\"round\"] > 10].reset_index(drop=True)\n",
    "data = data2[columns_mit_fe]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "X_train, X_test, y_train_hg, y_test_hg, y_train_ag, y_test_ag = get_season(data, 18)\n",
    "\n",
    "del X_train[\"season\"]\n",
    "del X_test[\"season\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=4, min_child_weight=1, missing=None, n_estimators=50,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#home\n",
    "model_h = XGBRegressor(n_estimators=50, max_depth=4)\n",
    "model_h.fit(X_train, y_train_hg)\n",
    "\n",
    "#away\n",
    "model_a = XGBRegressor(n_estimators=50, max_depth=4)\n",
    "model_a.fit(X_train, y_train_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = model_h.predict(X_test)\n",
    "away = model_a.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "test_df = data2.iloc[data[data[\"season\"] == 18].index]\n",
    "test_df.reset_index(drop=True, inplace=True)\n",
    "#ann_pred = pd.DataFrame(y_pred, columns = [\"H_pred_Goals\", \"A_pred_Goals\"])\n",
    "test_df[\"H_pred_Goals\"] = home\n",
    "test_df[\"A_pred_Goals\"] = away\n",
    "#test_df2[\"rps_Clf\"] = rps(test_df2[\"H_pred_Clf\"], test_df2[\"D_pred_Clf\"], test_df2[\"A_pred_Clf\"], test_df2[\"Home\"], test_df2[\"Draw\"], test_df2[\"Away\"])\n",
    "#test_df2[\"rps_ANN\"] = rps(test_df2[\"H_pred_Ann\"], test_df2[\"D_pred_Ann\"], test_df2[\"A_pred_Ann\"], test_df2[\"Home\"], test_df2[\"Draw\"], test_df2[\"Away\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Konny\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "test_df[\"XGB_pred_hc\"] = test_df[\"A_pred_Goals\"] - test_df[\"H_pred_Goals\"]\n",
    "test_df[\"Hc_Diff\"] = test_df[\"BbAHh\"] - test_df[\"XGB_pred_hc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    58.000000\n",
       " mean      0.031810\n",
       " std       0.888714\n",
       " min      -1.000000\n",
       " 25%      -1.000000\n",
       " 50%       0.000000\n",
       " 75%       0.940000\n",
       " max       1.220000\n",
       " Name: H_Ahc_PL, dtype: float64, count    27.000000\n",
       " mean      0.527037\n",
       " std       1.676821\n",
       " min      -1.000000\n",
       " 25%      -1.000000\n",
       " 50%       0.440000\n",
       " 75%       1.135000\n",
       " max       6.550000\n",
       " Name: H_PL, dtype: float64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[(test_df[\"Hc_Diff\"] > 0.1) & (test_df[\"BbAvAHH\"] > 1.9)].H_Ahc_PL.describe(), test_df[test_df[\"Hc_Diff\"] > 0.5].H_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    44.000000\n",
       " mean     -0.156591\n",
       " std       0.858646\n",
       " min      -1.000000\n",
       " 25%      -1.000000\n",
       " 50%      -0.500000\n",
       " 75%       0.635000\n",
       " max       1.150000\n",
       " Name: A_Ahc_PL, dtype: float64, count    17.000000\n",
       " mean     -0.503529\n",
       " std       0.861945\n",
       " min      -1.000000\n",
       " 25%      -1.000000\n",
       " 50%      -1.000000\n",
       " 75%       0.240000\n",
       " max       1.850000\n",
       " Name: A_PL, dtype: float64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[(test_df[\"Hc_Diff\"] < -0.1) & (test_df[\"BbAvAHA\"] > 1.9)].A_Ahc_PL.describe(), test_df[test_df[\"Hc_Diff\"] < -0.5].A_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    280.000000\n",
       "mean      -0.066357\n",
       "std        0.850441\n",
       "min       -1.000000\n",
       "25%       -1.000000\n",
       "50%        0.000000\n",
       "75%        0.810000\n",
       "max        1.230000\n",
       "Name: A_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"A_Ahc_PL\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1.039091\n",
       "1      1.028611\n",
       "2      1.031283\n",
       "3      1.033764\n",
       "4      1.030245\n",
       "5      1.039428\n",
       "6      1.032593\n",
       "7      1.033151\n",
       "8      1.034759\n",
       "9      1.033674\n",
       "10     1.035731\n",
       "11     1.032542\n",
       "12     1.037988\n",
       "13     1.034759\n",
       "14     1.033599\n",
       "15     1.034759\n",
       "16     1.031366\n",
       "17     1.034887\n",
       "18     1.033255\n",
       "19     1.035590\n",
       "20     1.035310\n",
       "21     1.036545\n",
       "22     1.031885\n",
       "23     1.034253\n",
       "24     1.032272\n",
       "25     1.037107\n",
       "26     1.031915\n",
       "27     1.033930\n",
       "28     1.034759\n",
       "29     1.036545\n",
       "         ...   \n",
       "250    1.037255\n",
       "251    1.038968\n",
       "252    1.034759\n",
       "253    1.042119\n",
       "254    1.033151\n",
       "255    1.035578\n",
       "256    1.040111\n",
       "257    1.033654\n",
       "258    1.041280\n",
       "259    1.035590\n",
       "260    1.034427\n",
       "261    1.036269\n",
       "262    1.036269\n",
       "263    1.036297\n",
       "264    1.035731\n",
       "265    1.039136\n",
       "266    1.042225\n",
       "267    1.039647\n",
       "268    1.037272\n",
       "269    1.039059\n",
       "270    1.039136\n",
       "271    1.039024\n",
       "272    1.036297\n",
       "273    1.039529\n",
       "274    1.039428\n",
       "275    1.039059\n",
       "276    1.036381\n",
       "277    1.036965\n",
       "278    1.037634\n",
       "279    1.036089\n",
       "Length: 280, dtype: float64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/test_df[\"BbAvAHH\"] + 1/test_df[\"BbAvAHA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = pd.read_csv(\"England_all_16.csv\")\n",
    "fra = pd.read_csv(\"France_all_16.csv\")\n",
    "ita = pd.read_csv(\"Italy_all_16.csv\")\n",
    "ger = pd.read_csv(\"Germany_all_16.csv\")\n",
    "spa = pd.read_csv(\"Spain_all_16.csv\")\n",
    "\n",
    "data = pd.concat([eng, fra, ita, ger, spa])\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rps_Book</th>\n",
       "      <th>rps_Rdf</th>\n",
       "      <th>rps_Xgb</th>\n",
       "      <th>rps_ANN</th>\n",
       "      <th>rps_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1326.000000</td>\n",
       "      <td>1326.000000</td>\n",
       "      <td>1326.000000</td>\n",
       "      <td>1326.000000</td>\n",
       "      <td>1326.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.196054</td>\n",
       "      <td>0.198840</td>\n",
       "      <td>0.201671</td>\n",
       "      <td>0.196002</td>\n",
       "      <td>0.197318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.134795</td>\n",
       "      <td>0.130556</td>\n",
       "      <td>0.144134</td>\n",
       "      <td>0.134966</td>\n",
       "      <td>0.133658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002399</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.006165</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.116891</td>\n",
       "      <td>0.120541</td>\n",
       "      <td>0.104724</td>\n",
       "      <td>0.119066</td>\n",
       "      <td>0.115173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.159694</td>\n",
       "      <td>0.165702</td>\n",
       "      <td>0.161436</td>\n",
       "      <td>0.162614</td>\n",
       "      <td>0.163399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.253820</td>\n",
       "      <td>0.254590</td>\n",
       "      <td>0.262079</td>\n",
       "      <td>0.253647</td>\n",
       "      <td>0.251659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.843500</td>\n",
       "      <td>0.866656</td>\n",
       "      <td>0.803792</td>\n",
       "      <td>0.843625</td>\n",
       "      <td>0.835653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rps_Book      rps_Rdf      rps_Xgb      rps_ANN      rps_avg\n",
       "count  1326.000000  1326.000000  1326.000000  1326.000000  1326.000000\n",
       "mean      0.196054     0.198840     0.201671     0.196002     0.197318\n",
       "std       0.134795     0.130556     0.144134     0.134966     0.133658\n",
       "min       0.002399     0.003040     0.001193     0.006165     0.003648\n",
       "25%       0.116891     0.120541     0.104724     0.119066     0.115173\n",
       "50%       0.159694     0.165702     0.161436     0.162614     0.163399\n",
       "75%       0.253820     0.254590     0.262079     0.253647     0.251659\n",
       "max       0.843500     0.866656     0.803792     0.843625     0.835653"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"rps_Book\", \"rps_Rdf\", \"rps_Xgb\", \"rps_ANN\", \"rps_avg\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    247.000000\n",
       "mean       0.019717\n",
       "std        0.882859\n",
       "min       -1.000000\n",
       "25%       -1.000000\n",
       "50%        0.000000\n",
       "75%        0.890000\n",
       "max        1.350000\n",
       "Name: H_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = 0.1\n",
    "\n",
    "data[data[\"H_pred_avg\"] / data[\"H_prob_odds\"] - 1 > value].H_Ahc_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    270.000000\n",
       "mean      -0.033296\n",
       "std        0.866043\n",
       "min       -1.000000\n",
       "25%       -1.000000\n",
       "50%        0.000000\n",
       "75%        0.840000\n",
       "max        1.220000\n",
       "Name: A_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"A_pred_avg\"] / data[\"A_prob_odds\"] - 1 > value].A_Ahc_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    256.000000\n",
       "mean       0.085391\n",
       "std        0.840537\n",
       "min       -1.000000\n",
       "25%       -1.000000\n",
       "50%        0.490000\n",
       "75%        0.850000\n",
       "max        1.180000\n",
       "Name: H_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference = 0.2\n",
    "\n",
    "data[data[\"Hc_Diff_avg\"] > difference].H_Ahc_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    375.000000\n",
       "mean       0.018480\n",
       "std        0.866511\n",
       "min       -1.000000\n",
       "25%       -1.000000\n",
       "50%        0.340000\n",
       "75%        0.840000\n",
       "max        1.180000\n",
       "Name: A_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"Hc_Diff_avg\"] < -difference].A_Ahc_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    237.000000\n",
       "mean      -0.018333\n",
       "std        0.848843\n",
       "min       -1.000000\n",
       "25%       -1.000000\n",
       "50%        0.000000\n",
       "75%        0.840000\n",
       "max        1.350000\n",
       "Name: H_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference = 0.05\n",
    "value = 0.05\n",
    "\n",
    "data[(data[\"Hc_Diff_avg\"] > difference) & (data[\"H_pred_avg\"] / data[\"H_prob_odds\"] - 1 > value)].H_Ahc_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    270.000000\n",
       "mean       0.024593\n",
       "std        0.873342\n",
       "min       -1.000000\n",
       "25%       -1.000000\n",
       "50%        0.000000\n",
       "75%        0.860000\n",
       "max        1.220000\n",
       "Name: A_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[(data[\"Hc_Diff_avg\"] < -difference) & (data[\"A_pred_avg\"] / data[\"A_prob_odds\"] - 1 > value)].A_Ahc_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----diff = 0-----value = 0-------\n",
      "Home: 0.004555808656036452 , 439.0\n",
      "Away: 0.009262048192771077 , 332.0\n",
      "Total: 0.006582360570687419 , 771.0\n",
      "-----diff = 0-----value = 0.05-------\n",
      "Home: -0.017292307692307694 , 325.0\n",
      "Away: 0.06381632653061221 , 245.0\n",
      "Total: 0.01757017543859648 , 570.0\n",
      "-----diff = 0-----value = 0.1-------\n",
      "Home: -0.01673728813559322 , 236.0\n",
      "Away: 0.1279234972677596 , 183.0\n",
      "Total: 0.04644391408114559 , 419.0\n",
      "-----diff = 0-----value = 0.15-------\n",
      "Home: -0.018898809523809512 , 168.0\n",
      "Away: 0.1695833333333333 , 132.0\n",
      "Total: 0.06403333333333333 , 300.0\n",
      "-----diff = 0-----value = 0.2-------\n",
      "Home: -0.06965811965811965 , 117.0\n",
      "Away: 0.16451086956521732 , 92.0\n",
      "Total: 0.03342105263157893 , 209.0\n",
      "-----diff = 0-----value = 0.25-------\n",
      "Home: -0.07852272727272726 , 88.0\n",
      "Away: 0.14171428571428563 , 70.0\n",
      "Total: 0.01905063291139238 , 158.0\n",
      "-----diff = 0.05-----value = 0-------\n",
      "Home: 0.010346666666666683 , 375.0\n",
      "Away: 0.04861111111111109 , 306.0\n",
      "Total: 0.027540381791483118 , 681.0\n",
      "-----diff = 0.05-----value = 0.05-------\n",
      "Home: 0.00010600706713782105 , 283.0\n",
      "Away: 0.09703463203463203 , 231.0\n",
      "Total: 0.04366731517509728 , 514.0\n",
      "-----diff = 0.05-----value = 0.1-------\n",
      "Home: -0.012089201877934273 , 213.0\n",
      "Away: 0.15051724137931038 , 174.0\n",
      "Total: 0.06102067183462535 , 387.0\n",
      "-----diff = 0.05-----value = 0.15-------\n",
      "Home: -0.02261437908496732 , 153.0\n",
      "Away: 0.19290697674418608 , 129.0\n",
      "Total: 0.07597517730496456 , 282.0\n",
      "-----diff = 0.05-----value = 0.2-------\n",
      "Home: -0.06108108108108109 , 111.0\n",
      "Away: 0.18483333333333327 , 90.0\n",
      "Total: 0.04902985074626862 , 201.0\n",
      "-----diff = 0.05-----value = 0.25-------\n",
      "Home: -0.0727710843373494 , 83.0\n",
      "Away: 0.14171428571428563 , 70.0\n",
      "Total: 0.02535947712418297 , 153.0\n",
      "-----diff = 0.1-----value = 0-------\n",
      "Home: 0.01810344827586208 , 319.0\n",
      "Away: 0.05261992619926196 , 271.0\n",
      "Total: 0.033957627118644064 , 590.0\n",
      "-----diff = 0.1-----value = 0.05-------\n",
      "Home: -0.0037346938775509922 , 245.0\n",
      "Away: 0.10108374384236453 , 203.0\n",
      "Total: 0.04376116071428573 , 448.0\n",
      "-----diff = 0.1-----value = 0.1-------\n",
      "Home: -0.048930481283422464 , 187.0\n",
      "Away: 0.15610389610389613 , 154.0\n",
      "Total: 0.043665689149560136 , 341.0\n",
      "-----diff = 0.1-----value = 0.15-------\n",
      "Home: -0.055036764705882354 , 136.0\n",
      "Away: 0.21750000000000005 , 118.0\n",
      "Total: 0.07157480314960632 , 254.0\n",
      "-----diff = 0.1-----value = 0.2-------\n",
      "Home: -0.10939999999999998 , 100.0\n",
      "Away: 0.23932926829268283 , 82.0\n",
      "Total: 0.04771978021978019 , 182.0\n",
      "-----diff = 0.1-----value = 0.25-------\n",
      "Home: -0.08875000000000001 , 80.0\n",
      "Away: 0.18537313432835814 , 67.0\n",
      "Total: 0.03619047619047615 , 147.0\n",
      "-----diff = 0.15-----value = 0-------\n",
      "Home: 0.007405303030303037 , 264.0\n",
      "Away: 0.06159999999999997 , 225.0\n",
      "Total: 0.032341513292433524 , 489.0\n",
      "-----diff = 0.15-----value = 0.05-------\n",
      "Home: -0.01080188679245281 , 212.0\n",
      "Away: 0.08882352941176469 , 170.0\n",
      "Total: 0.033534031413612574 , 382.0\n",
      "-----diff = 0.15-----value = 0.1-------\n",
      "Home: -0.053734939759036156 , 166.0\n",
      "Away: 0.1345384615384615 , 130.0\n",
      "Total: 0.02895270270270268 , 296.0\n",
      "-----diff = 0.15-----value = 0.15-------\n",
      "Home: -0.052399999999999995 , 125.0\n",
      "Away: 0.18698019801980192 , 101.0\n",
      "Total: 0.05457964601769909 , 226.0\n",
      "-----diff = 0.15-----value = 0.2-------\n",
      "Home: -0.10635869565217392 , 92.0\n",
      "Away: 0.2316197183098591 , 71.0\n",
      "Total: 0.04085889570552145 , 163.0\n",
      "-----diff = 0.15-----value = 0.25-------\n",
      "Home: -0.09313333333333333 , 75.0\n",
      "Away: 0.17389830508474574 , 59.0\n",
      "Total: 0.02444029850746267 , 134.0\n",
      "-----diff = 0.2-----value = 0-------\n",
      "Home: -0.004635922330097082 , 206.0\n",
      "Away: 0.0443041237113402 , 194.0\n",
      "Total: 0.0191 , 400.0\n",
      "-----diff = 0.2-----value = 0.05-------\n",
      "Home: -0.011704545454545441 , 176.0\n",
      "Away: 0.07099315068493152 , 146.0\n",
      "Total: 0.02579192546583852 , 322.0\n",
      "-----diff = 0.2-----value = 0.1-------\n",
      "Home: -0.0706164383561644 , 146.0\n",
      "Away: 0.11376106194690262 , 113.0\n",
      "Total: 0.009826254826254805 , 259.0\n",
      "-----diff = 0.2-----value = 0.15-------\n",
      "Home: -0.05157894736842106 , 114.0\n",
      "Away: 0.17022988505747125 , 87.0\n",
      "Total: 0.044427860696517406 , 201.0\n",
      "-----diff = 0.2-----value = 0.2-------\n",
      "Home: -0.1186627906976744 , 86.0\n",
      "Away: 0.2060833333333333 , 60.0\n",
      "Total: 0.014794520547945207 , 146.0\n",
      "-----diff = 0.2-----value = 0.25-------\n",
      "Home: -0.12390410958904106 , 73.0\n",
      "Away: 0.17301886792452828 , 53.0\n",
      "Total: 0.000992063492063492 , 126.0\n",
      "-----diff = 0.25-----value = 0-------\n",
      "Home: -0.038647058823529375 , 170.0\n",
      "Away: 0.0515204678362573 , 171.0\n",
      "Total: 0.006568914956011744 , 341.0\n",
      "-----diff = 0.25-----value = 0.05-------\n",
      "Home: -0.046300000000000015 , 150.0\n",
      "Away: 0.08722222222222221 , 126.0\n",
      "Total: 0.014655797101449262 , 276.0\n",
      "-----diff = 0.25-----value = 0.1-------\n",
      "Home: -0.11318897637795274 , 127.0\n",
      "Away: 0.1592929292929293 , 99.0\n",
      "Total: 0.006172566371681422 , 226.0\n",
      "-----diff = 0.25-----value = 0.15-------\n",
      "Home: -0.10625000000000001 , 100.0\n",
      "Away: 0.209025974025974 , 77.0\n",
      "Total: 0.03090395480225987 , 177.0\n",
      "-----diff = 0.25-----value = 0.2-------\n",
      "Home: -0.14077922077922078 , 77.0\n",
      "Away: 0.22564814814814813 , 54.0\n",
      "Total: 0.010267175572519075 , 131.0\n",
      "-----diff = 0.25-----value = 0.25-------\n",
      "Home: -0.16430769230769224 , 65.0\n",
      "Away: 0.2259183673469387 , 49.0\n",
      "Total: 0.0034210526315789523 , 114.0\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "    for j in [0, 0.05, 0.1, 0.15, 0.2, 0.25]:\n",
    "        h = data[(data[\"Hc_Diff_avg\"] > i) & (data[\"H_pred_avg\"] / data[\"H_prob_odds\"] - 1 > j)].H_Ahc_PL.describe()\n",
    "        a = data[(data[\"Hc_Diff_avg\"] < -i) & (data[\"A_pred_avg\"] / data[\"A_prob_odds\"] - 1 > j)].A_Ahc_PL.describe()\n",
    "        print(f\"-----diff = {i}-----value = {j}-------\")\n",
    "        print(f\"Home: {h['mean']} , {h['count']}\")\n",
    "        print(f\"Away: {a['mean']} , {a['count']}\")\n",
    "        print(f\"Total: {(h['mean']*h['count']+a['mean']*a['count'])/(h['count']+a['count'])} , {h['count']+a['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27.0</td>\n",
       "      <td>0.302037</td>\n",
       "      <td>0.758453</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.7800</td>\n",
       "      <td>0.86000</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>37.0</td>\n",
       "      <td>0.066892</td>\n",
       "      <td>0.793321</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.79000</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>33.0</td>\n",
       "      <td>0.155455</td>\n",
       "      <td>0.545430</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.32000</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.138194</td>\n",
       "      <td>0.829570</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.77250</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>37.0</td>\n",
       "      <td>-0.004865</td>\n",
       "      <td>0.931825</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.91000</td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31.0</td>\n",
       "      <td>-0.115645</td>\n",
       "      <td>0.868415</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.77500</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.006667</td>\n",
       "      <td>0.911605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.87000</td>\n",
       "      <td>1.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.0</td>\n",
       "      <td>0.218429</td>\n",
       "      <td>0.913317</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.97500</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>42.0</td>\n",
       "      <td>0.003333</td>\n",
       "      <td>0.950898</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.2325</td>\n",
       "      <td>0.92500</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>37.0</td>\n",
       "      <td>-0.188378</td>\n",
       "      <td>0.890114</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.81000</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30.0</td>\n",
       "      <td>-0.266167</td>\n",
       "      <td>0.853801</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>0.68875</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std  min   25%     50%      75%   max\n",
       "season                                                             \n",
       "8        27.0  0.302037  0.758453 -1.0 -0.25  0.7800  0.86000  1.06\n",
       "9        37.0  0.066892  0.793321 -1.0 -1.00  0.2000  0.79000  1.07\n",
       "10       33.0  0.155455  0.545430 -1.0  0.03  0.1000  0.32000  1.00\n",
       "11       36.0 -0.138194  0.829570 -1.0 -1.00  0.0000  0.77250  1.12\n",
       "12       37.0 -0.004865  0.931825 -1.0 -1.00  0.0000  0.91000  1.11\n",
       "13       31.0 -0.115645  0.868415 -1.0 -1.00  0.0000  0.77500  1.04\n",
       "14       36.0 -0.006667  0.911605 -1.0 -1.00  0.0000  0.87000  1.20\n",
       "15       35.0  0.218429  0.913317 -1.0 -1.00  0.7500  0.97500  1.22\n",
       "16       42.0  0.003333  0.950898 -1.0 -1.00  0.2325  0.92500  1.22\n",
       "17       37.0 -0.188378  0.890114 -1.0 -1.00 -1.0000  0.81000  1.31\n",
       "18       30.0 -0.266167  0.853801 -1.0 -1.00 -1.0000  0.68875  1.16"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[(df1[\"A_Form_Tot4\"] >= 1.25) & (df1[\"H_prob_odds\"] >= 0.5)].groupby(\"season\").H_Ahc_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1156.000000\n",
       "mean       -0.049079\n",
       "std         0.914060\n",
       "min        -1.000000\n",
       "25%        -1.000000\n",
       "50%         0.000000\n",
       "75%         0.860000\n",
       "max         7.490000\n",
       "Name: A_Ahc_PL, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1[\"H_Form_Tot4\"] >= 1.2].A_Ahc_PL.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
